# Quick Optimization Reference Guide

## üéØ Quick Model Selection by VRAM

| Your GPU VRAM | Recommended Model | Config | Expected VRAM |
|---------------|-------------------|--------|---------------|
| **4 GB** (A16 1/4) | `large-v3-turbo` | INT8 | **~2.1 GB** ‚úÖ |
| **4 GB** (More headroom) | `medium` | INT8 | ~1-1.5 GB |
| **2 GB** | `small` | INT8 | ~500 MB |
| **1 GB or less** | `base` | INT8 | ~150 MB |

## üîß Quick Configuration Templates

### 1. 4GB GPU - Balanced (Current Best ‚úÖ)
```env
WHISPER_MODEL_SIZE=large-v3-turbo
DEVICE_TYPE=cuda
WL_COMPUTE_TYPE=int8
WL_MAX_CLIENTS=5-8
```
**Result:** ~2.1 GB VRAM (validated)

### 2. 4GB GPU - More Headroom
```env
WHISPER_MODEL_SIZE=medium
DEVICE_TYPE=cuda
WL_COMPUTE_TYPE=int8
WL_MAX_CLIENTS=10+
```
**Result:** ~1-1.5 GB VRAM

### 3. 2GB GPU - Minimal
```env
WHISPER_MODEL_SIZE=small
DEVICE_TYPE=cuda
WL_COMPUTE_TYPE=int8
WL_MAX_CLIENTS=5-8
```
**Result:** ~500 MB VRAM

### 4. CPU-Only - Balanced
```env
WHISPER_MODEL_SIZE=medium
DEVICE_TYPE=cpu
WL_COMPUTE_TYPE=int8
WL_CPU_THREADS=4
```
**Result:** ~2-4 GB RAM

### 5. CPU-Only - Minimal
```env
WHISPER_MODEL_SIZE=small
DEVICE_TYPE=cpu
WL_COMPUTE_TYPE=int8
WL_CPU_THREADS=4
```
**Result:** ~1-2 GB RAM

## üìä All Multilingual Models Comparison

| Model | GPU VRAM (INT8) | CPU RAM (INT8) | Quality | Speed |
|-------|-----------------|----------------|---------|-------|
| **large-v3-turbo** | ~2.1 GB ‚úÖ | ~6-8 GB | Excellent | Very Fast |
| **medium** | ~1-1.5 GB | ~2-4 GB | Excellent | Fast |
| **small** | ~500 MB | ~1-2 GB | Very Good | Very Fast |
| **base** | ~150 MB | ~300-600 MB | Good | Extremely Fast |
| **tiny** | ~75 MB | ~150-300 MB | Basic | Extremely Fast |

**All models above are multilingual (99+ languages)** ‚úÖ

## ‚ö° Quick Optimization Tips

### Reduce VRAM Further:
1. ‚úÖ Use INT8 quantization (already done)
2. ‚úÖ Use smaller model (`medium` ‚Üí `small`)
3. ‚úÖ Reduce `WL_MAX_CLIENTS`
4. ‚úÖ Use greedy decoding (`beam_size=1`)

### Reduce CPU RAM:
1. ‚úÖ Use INT8 quantization
2. ‚úÖ Use smaller model
3. ‚úÖ Configure `WL_CPU_THREADS` appropriately

### Improve Speed:
1. ‚úÖ Use GPU (vs CPU)
2. ‚úÖ Use INT8 quantization
3. ‚úÖ Use `large-v3-turbo` (faster than large-v3)
4. ‚úÖ Reduce `beam_size` to 1 (greedy)

## üöÄ Quick Test Commands

### Test Medium Model:
```bash
# Update .env
WHISPER_MODEL_SIZE=medium
WL_COMPUTE_TYPE=int8

# Rebuild and test
docker compose --profile gpu build whisperlive
docker compose --profile gpu up -d whisperlive

# Check VRAM
nvidia-smi
```

### Test Small Model:
```bash
# Update .env
WHISPER_MODEL_SIZE=small
WL_COMPUTE_TYPE=int8

# Rebuild and test
docker compose --profile gpu build whisperlive
docker compose --profile gpu up -d whisperlive

# Check VRAM
nvidia-smi
```

## üìã Validation Checklist

After changing model, verify:
- [ ] Model loads successfully
- [ ] Check VRAM usage: `nvidia-smi`
- [ ] Check logs for correct model/precision
- [ ] Test transcription quality
- [ ] Test latency/performance
- [ ] Monitor for memory issues

## üîç Current Status

‚úÖ **Currently Using:**
- Model: `large-v3-turbo`
- Device: `cuda` (GPU)
- Compute: `int8`
- VRAM: **~2.1 GB** (validated)

‚úÖ **Ready to Test:**
- `medium` + INT8 ‚Üí ~1-1.5 GB VRAM
- `small` + INT8 ‚Üí ~500 MB VRAM

---

**See `COMPREHENSIVE_OPTIMIZATION_RESEARCH.md` for full details.**

